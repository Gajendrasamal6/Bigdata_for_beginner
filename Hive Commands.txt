RDBMS:
	store "live" data
	OLTP - OnLine Transaction Processing
	To process this data:
		analyze
		generate reports
		forecasting
		
		will take a lot of resources:
			CPU
			RAM
			disk space
		hamper the performance of the DBMS itself

	Customers
	Products
	Categories
	Orders
	OrderDetails
	
Solution:
	transfer the "relevant" data to another DBMS server for processing only!
	perform all processing on this "other" system / server
	OLTP is not affected
	processing can be done in parallel to the OLTP
	OLAP: OnLine Analytical Processing	- DATA WAREHOUSE
	de-normalized
	ETL: Extract-Transform-Load
	
SQL Server - OLTP
SQL Server - OLAP
	SSAS - SQL Server Analytical Services
	SSIS - SQL Server Integration Services (ETL)
	
	OLAP data is de-normalized, you have minimal "joins" 
	customerId firstname lastname city orderid orderyear ordermonth productid productname categoryid categoryname qty rate price

What were the sales in the year 2019?
What were the sales in the month of June of 2020?
What were the sales in the city of mumbai?

JDBC: Java DataBase Connectivity
ODBC: Open DataBase Connectivity

Partitions:
	Tables:
		Customers
		Products
		Categories
		Orders
		OrderDetails
	Amazon India is genreating 1M records
	analytics / generate reports / view data for specific cities
	SELECT * FROM amazonorders
	WHERE City = 'Mumbai'
		250000 => Mumbai
	Split the table into small sub-tables based on "city"
	create partitions on the amaxzonorders table based on the "city" column
		determine unique cities
		and split the data into that many partitions (sub-tables)
			10 cities
			mumbai partition will only have data for mumbai orders
			delhi partition will only have data for delhi orders
			SELECT * FROM amazonorders
			WHERE City = 'Mumbai'
	1M records
	500,000 records => insert into amazonorders
	partitions -> determines unique data
Buckets:
	splitting data further into smaller buckets
	amazonorders
		partitions based on cities
		can have huge volumes of data
			image 250k records daily
			pincode or quantity
			NOT going to determine "uniqueness" to split the data
			applies a formula
				create 3 buckets based on pincode
				reads every record,
				picks the pincode
				applies a "hash" to the data (pincode) and comes up with a value (F(x))
				F(x) % 3
					3 % 3 = 0 => B0
				
				B0
				B1
				B2
			
			SELECT * FROM tablename
			WHERE pincode = "400067"
			
				take 4000067 -> apply a hash to it - gives F(x) - divide F(x) by no. buckets configured (3) -> determine modulo - fetch data from that specific (modulo) bucket.
				for e.g.; 
					400067 -> 1, fetch data from B1
					400101 -> 2, fetch data from B2
					400103 -> 0, fetch data from B0
				
		you can create buckets without having partitions

SHOW DataBases;
USE <dbname>;
SHOW TABLES;
CREATE Database <db_name>;
DROP DATABASE dbname;
DROP DATABASE IF EXISTS dbname;
DROP DATABASE IF EXISTS dbname CASCADE;
CREATE Database IF NOT EXISTS <db_name>;
CREATE Database IF NOT EXISTS <db_name> WITH DBPROPERTIES ('key1' = 'value1', 'key2' = 'value2',...'keyn' = 'valuen');
DESCRIBE Database dbname;
DESCRIBE Database EXTENDED dbname;

Hive has 2 types of tables:
	Internal
	External
	
USE demo;
CREATE TABLE employee (Id int, Name string, Salary float)
row format delimited
fields terminated by ',';

CREATE TABLE IF NOT EXISTS new_employee (Id int COMMENT 'Employee Id', Name string COMMENT 'Name of the employee', Salary float COMMENT 'Employee Salary' )
row format delimited
fields terminated by ','
TBLPROPERTIES ('creator'='Ajay Singala', 'date'='28th July 2021');

DESC new_employee;
DESC EXTENDED new_employee;

CREATE TABLE employee_clone LIKE Employee;

DROP TABLE employee_clone;

# External Tables.
hdfs dfs -mkdir /hivedirectory
hdfs dfs -put ./emp_details /hivedirectory

CREATE EXTERNAL TABLE emplist (id int, name string, salary float)
row format delimited
fields terminated by ','
location '/hivedirectory';

SELECT * FROM emplist;

hdfs dfs -put ./emp_details2 /hivedirectory/
SELECT * FROM emplist;

hdfs dfs -put ./dummy /hivedirectory/
SELECT * FROM emplist;
hdfs dfs -rm /hivedirectory/dummy

#Loading data into Hive.
LOAD DATA INPATH '/hivedirectory/emp_details'
INTO TABLE demo.employee_clone;

LOAD DATA INPATH '/hivedirectory'
INTO TABLE demo.employee_clone;

LOAD DATA LOCAL INPATH '<path on local>'
INTO TABLE demo.employee_clone;

# ALTER TABLE.
# Rename
ALTER TABLE existing_table RENAME TO new_tablename;

# Add columns.
ATLER TABLE tablename
ADD COLUMNS(new_columnname data_type);

ATLER TABLE emp
ADD COLUMNS(city string);
ATLER TABLE emp
ADD COLUMNS(age int);

# Modify an existing column.
ATLER TABLE tablename
CHANGE old_column new_coumn datatype;

ATLER TABLE emp
CHANGE DateOfBirth DOB string;

# Delete or Replace a column.
ALTER TABLE tablename
REPLACE columns(new_column1 datatype, new_column2 datatype, new_columnn datatype);

emp table BEFORE:
id string
name string
salary float
age int

ALTER TABLE tablename
REPLACE columns(id string, first_name string, age int);

emp table AFTER:
id string
first_name string
age int

# Splitting.
SELECT emp.name, split(emp.name, ' ')
FROM emp;

Ajay Singala, ["Ajay", "Singala"]

SELECT emp.name, split(emp.name, ' ')[0] AS Firstname
FROM emp;

Ajay Singala, Ajay

# Partitioning
# Static Partitioning.
CREATE TABLE Students (id int, name string, age int)
partitioned by (course string)
row format delimited
fields terminated by ',';

hdfs dfs -put ./students.txt /hivedirectory

LOAD DATA INPATH '/hivedirectory/students_bigdata.txt'
INTO TABLE demo.students
partition(course='Big Data');

SHOW PARTITIONS Students;

LOAD DATA INPATH '/hivedirectory/students_java.txt'
INTO TABLE demo.students
partition(course='Java');

SHOW PARTITIONS Students;

INSERT INTO Students Partition(course="Python") VALUES (1207, 'stark', 44) ;

SHOW PARTITIONS Students;

# Dynamic Partitioning.
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

CREATE TABLE Students_dp (id int, name string, age int, course string)
row format delimited
fields terminated by ',';

LOAD DATA INPATH '/hivedirectory/students.txt'
INTO TABLE students_dp;

CREATE TABLE Students_dp_part (id int, name string, age int)
partitioned by (course string)
row format delimited
fields terminated by ',';

INSERT INTO Students_dp_part 
partition(course)
SELECT id, name, age, course
FROM Students_dp;

SHOW PARTITIONS Students_dp_part;

SELECT * FROM Students_dp_part;
SELECT * FROM Students_dp_part WHERE course='Big Data';
SELECT * FROM Students_dp_part WHERE course='Java';

# Bucketing.
set hive.enforce.bucketing = true;

CREATE TABLE empbucket (id int, name string, salary float)
CLUSTERED BY (id) INTO 3 Buckets
row format delimited
fields terminated by ',';

INSERT OVERWRITE TABLE EmpBucket
SELECT * FROM employee_clone;

hdfs dfs -cat /user/hive.warehouse/showbucket.db/empbucket/00000_0;
hdfs dfs -cat /user/hive.warehouse/showbucket.db/empbucket/00001_0;
hdfs dfs -cat /user/hive.warehouse/showbucket.db/empbucket/00002_0;

# HiveQL
# Operators
# Arithmetic:
	+
	-
	*
	/
	%
	
SELECT OrderId, ProductId, Qty, Rate, (Qty * Rate) AS Price
FROM OrdreDetails;

# Comparison Operators:
=
<>
!=
<
<=
>
>=
IS NULL
IS NOT NULL

# HQL functions:
split()

# Mathematical Functions:
round(number)
sqrt(num)
abs(num)

# Aggregate functions:
count(*)
sum(column)
avg(col)
min(col)
max(col)

# Other functions.
length(str)
concat(str1, str2,...)
substr(str, start_index)
substr(str, start_index, length)
upper(str)
lower(str)
trim(str)
ltrim(str)
rtrim(str)

# HQL GROUP BY and HAVING clauses
SELECT Course, COUNT(*)
FROM students
GROUP BY Course;

SELECT State, City, SUM(Sales_amt)
FROM SalesData
GROUP BY State, City

SELECT State, City, SUM(Sales_amt) AS TotalSales
FROM SalesData
GROUP BY State, City
HAVING TotalSales > 100000

# ORDER BY
SELECT *
FROM students
ORDER BY Name;

SELECT *
FROM students
ORDER BY Name DESC;

# SORT BY.
# do the sorting on each reducer.
SELECT *
FROM students
SORT BY Name;

# JOINS
INNER JOIN
LEFT OUTER JOIN
RIGHT OUTER JOIN
FULL OUTER JOIN


Create 2 tables:
Employee:
	id
	name
	deptid
Department:
	id
	name


Department table:
1 IT
2 HR
3 Finance
4 Sales

Employee table
1 A 1
2 B 2
3 C 1
4 D 3
5 E 9
6 F 8


SELECT e.id, e.name, e.deptid,
d.id, d.name
FROM employee as e
INNER JOIN department as d ON e.deptid = d.id

SELECT e.id, e.name, e.deptid,
d.id, d.name
FROM department as d
LEFT OUTER JOIN employee as e ON d.id = e.deptid
1 IT		1 		A 	1
1 IT		3 		C 	1
2 HR		2 		B 	2
3 Finance	4 		D 	3
4 Sales		null  null	null


SELECT e.id, e.name, e.deptid,
d.id, d.name
FROM department as d
RIGHT OUTER JOIN employee as e ON d.id = e.deptid
1 A 1 1 IT
2 B 2 2 HR
3 C 1 1 IT
4 D 3 3 Finance
5 E 9 null null
6 F 8 null null

SELECT e.id, e.name, e.deptid,
d.id, d.name
FROM department as d
FULL OUTER JOIN employee as e ON d.id = e.deptid
1 A 1 1 IT
2 B 2 2 HR
3 C 1 1 IT
4 D 3 3 Finance
5 E 9 null null
6 F 8 null null
null null null 4 Sales
