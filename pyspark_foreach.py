# pyspark_foreach.py
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('ajaysingala.com').getOrCreate()

# Create a dataframe.
data = [('James','Smith','M',30),('Anna','Rose','F',41),
  ('Robert','Williams','M',62), 
]
columns = ["firstname","lastname","gender","salary"]
df = spark.createDataFrame(data=data, schema = columns)
df.show()

# Use select() or withColumn()
from pyspark.sql.functions import concat_ws,col,lit
df.select(concat_ws(",",df.firstname,df.lastname).alias("name"), \
          df.gender,lit(df.salary*2).alias("new_salary")).show()

# Use map() to loop Through Rows in DataFrame.
# Refering columns by index.
rdd=df.rdd.map(lambda x: 
    (x[0]+","+x[1],x[2],x[3]*2)
    )  
df2=rdd.toDF(["name","gender","new_salary"])
df2.show()

# Referring Column Names
rdd2=df.rdd.map(lambda x: 
    (x["firstname"]+","+x["lastname"],x["gender"],x["salary"]*2)
    ) 

# Alternative: Referring Column Names
rdd2=df.rdd.map(lambda x: 
    (x.firstname+","+x.lastname,x.gender,x.salary*2)
    ) 

# By Calling function
def func1(x):
    firstName=x.firstname
    lastName=x.lastName
    name=firstName+","+lastName
    gender=x.gender.lower()
    salary=x.salary*2
    return (name,gender,salary)

rdd2=df.rdd.map(lambda x: func1(x))

# Using foreach().
# Foreach example
def f(x): print(x)
df.foreach(f)

# Another example
df.foreach(lambda x: 
    print("Data ==>"+x["firstname"]+","+x["lastname"]+","+x["gender"]+","+str(x["salary"]*2))
    ) 

# # Using pandas
# import pandas as pd
# spark.conf.set("spark.sql.execution.arrow.enabled", "true")
# pandasDF = df.toPandas()
# for index, row in pandasDF.iterrows():
#     print(row['firstname'], row['gender'])

# # Collect Data As List and Loop Through.
# # Collect the data to Python List
# dataCollect = df.collect()
# for row in dataCollect:
#     print(row['firstname'] + "," +row['lastname'])

# #Using toLocalIterator()
# dataCollect=df.rdd.toLocalIterator()
# for row in dataCollect:
#     print(row['firstname'] + "," +row['lastname'])
